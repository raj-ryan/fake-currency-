{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4872904,"sourceType":"datasetVersion","datasetId":2825392}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch --upgrade\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T06:53:18.227109Z","iopub.execute_input":"2024-09-11T06:53:18.227717Z","iopub.status.idle":"2024-09-11T06:53:29.582974Z","shell.execute_reply.started":"2024-09-11T06:53:18.227682Z","shell.execute_reply":"2024-09-11T06:53:29.581762Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.5.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# Define number of classes\nnum_classes = 2  # Replace with your actual number of classes\n\nimport os\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define your transformations\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(40),\n        transforms.ColorJitter(brightness=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n# Set the data directory path\ndata_dir = '/kaggle/input/indian-currency-dataset/Indian Currency Dataset'\n\n# Create datasets using ImageFolder\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n\n# Create DataLoader for each dataset\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'test']}\n\n\n# Use CUDA if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load and modify the MobileNetV2 model\nmobilenet_model = models.mobilenet_v2(pretrained=True)\nmobilenet_model.classifier[1] = nn.Linear(mobilenet_model.last_channel, num_classes)\nmobilenet_model = mobilenet_model.to(device)\n\n# Load and modify the EfficientNetB0 model\nefficientnet_model = models.efficientnet_b0(pretrained=True)\nefficientnet_model.classifier[1] = nn.Linear(efficientnet_model.classifier[1].in_features, num_classes)\nefficientnet_model = efficientnet_model.to(device)\n\n# Training function\ndef train_model(model, criterion, optimizer, num_epochs=25):\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(image_datasets[phase])\n            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n    return model\n\n# Set up criterion and optimizers\ncriterion = nn.CrossEntropyLoss()\noptimizer_mobilenet = optim.Adam(mobilenet_model.parameters(), lr=0.001)\noptimizer_efficientnet = optim.Adam(efficientnet_model.parameters(), lr=0.001)\n\n# Train the models\nmobilenet_model = train_model(mobilenet_model, criterion, optimizer_mobilenet, num_epochs=25)\nefficientnet_model = train_model(efficientnet_model, criterion, optimizer_efficientnet, num_epochs=25)\n\n# Feature extraction function\ndef extract_features(model, dataloader):\n    model.eval()\n    features = []\n    labels = []\n\n    with torch.no_grad():\n        for inputs, labels_batch in dataloader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            features.append(outputs.cpu().numpy())\n            labels.append(labels_batch.numpy())\n\n    return np.concatenate(features), np.concatenate(labels)\n\n# Extract features using both models\nmobilenet_features_train, train_labels = extract_features(mobilenet_model, dataloaders['train'])\nmobilenet_features_val, val_labels = extract_features(mobilenet_model, dataloaders['test'])\nefficientnet_features_train, _ = extract_features(efficientnet_model, dataloaders['train'])\nefficientnet_features_val, _ = extract_features(efficientnet_model, dataloaders['test'])\n\n# Stack the features\ntrain_features = np.hstack((mobilenet_features_train, efficientnet_features_train))\nval_features = np.hstack((mobilenet_features_val, efficientnet_features_val))\n\n# Define classifiers\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\nlr = LogisticRegression(max_iter=1000)\nsvc = SVC(probability=True)\n\n# Train the classifiers\nrf.fit(train_features, train_labels)\ngb.fit(train_features, train_labels)\nlr.fit(train_features, train_labels)\nsvc.fit(train_features, train_labels)\n\n# Validate the classifiers\nrf_preds = rf.predict(val_features)\ngb_preds = gb.predict(val_features)\nlr_preds = lr.predict(val_features)\nsvc_preds = svc.predict(val_features)\n\n# Print accuracy scores\nprint(\"Random Forest Accuracy:\", accuracy_score(val_labels, rf_preds))\nprint(\"Gradient Boosting Accuracy:\", accuracy_score(val_labels, gb_preds))\nprint(\"Logistic Regression Accuracy:\", accuracy_score(val_labels, lr_preds))\nprint(\"SVC Accuracy:\", accuracy_score(val_labels, svc_preds))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-11T07:00:43.405344Z","iopub.execute_input":"2024-09-11T07:00:43.405694Z","iopub.status.idle":"2024-09-11T07:06:02.695989Z","shell.execute_reply.started":"2024-09-11T07:00:43.405665Z","shell.execute_reply":"2024-09-11T07:06:02.694624Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25\n----------\ntrain Loss: 0.7132 Acc: 0.6210\ntest Loss: 0.6132 Acc: 0.5981\nEpoch 2/25\n----------\ntrain Loss: 0.4777 Acc: 0.7863\ntest Loss: 0.9016 Acc: 0.7570\nEpoch 3/25\n----------\ntrain Loss: 0.4359 Acc: 0.7823\ntest Loss: 0.4527 Acc: 0.7477\nEpoch 4/25\n----------\ntrain Loss: 0.4017 Acc: 0.8266\ntest Loss: 0.8442 Acc: 0.7850\nEpoch 5/25\n----------\ntrain Loss: 0.5145 Acc: 0.7903\ntest Loss: 0.4640 Acc: 0.7383\nEpoch 6/25\n----------\ntrain Loss: 0.4389 Acc: 0.7944\ntest Loss: 0.3257 Acc: 0.8692\nEpoch 7/25\n----------\ntrain Loss: 0.3898 Acc: 0.8347\ntest Loss: 0.7360 Acc: 0.8131\nEpoch 8/25\n----------\ntrain Loss: 0.3363 Acc: 0.8871\ntest Loss: 0.2776 Acc: 0.8785\nEpoch 9/25\n----------\ntrain Loss: 0.4046 Acc: 0.8347\ntest Loss: 0.3715 Acc: 0.8224\nEpoch 10/25\n----------\ntrain Loss: 0.3842 Acc: 0.8427\ntest Loss: 0.2594 Acc: 0.8972\nEpoch 11/25\n----------\ntrain Loss: 0.3344 Acc: 0.8669\ntest Loss: 0.2491 Acc: 0.8598\nEpoch 12/25\n----------\ntrain Loss: 0.3518 Acc: 0.8387\ntest Loss: 0.2084 Acc: 0.9252\nEpoch 13/25\n----------\ntrain Loss: 0.3089 Acc: 0.8589\ntest Loss: 0.2368 Acc: 0.9252\nEpoch 14/25\n----------\ntrain Loss: 0.3312 Acc: 0.8992\ntest Loss: 0.2854 Acc: 0.9065\nEpoch 15/25\n----------\ntrain Loss: 0.2909 Acc: 0.8750\ntest Loss: 0.2303 Acc: 0.8972\nEpoch 16/25\n----------\ntrain Loss: 0.2652 Acc: 0.8992\ntest Loss: 0.2398 Acc: 0.9065\nEpoch 17/25\n----------\ntrain Loss: 0.2861 Acc: 0.8790\ntest Loss: 0.1987 Acc: 0.9159\nEpoch 18/25\n----------\ntrain Loss: 0.3493 Acc: 0.8145\ntest Loss: 0.2569 Acc: 0.8505\nEpoch 19/25\n----------\ntrain Loss: 0.2725 Acc: 0.8871\ntest Loss: 0.2084 Acc: 0.9159\nEpoch 20/25\n----------\ntrain Loss: 0.3081 Acc: 0.8629\ntest Loss: 0.2120 Acc: 0.9439\nEpoch 21/25\n----------\ntrain Loss: 0.3810 Acc: 0.8508\ntest Loss: 0.3561 Acc: 0.8972\nEpoch 22/25\n----------\ntrain Loss: 0.3145 Acc: 0.8750\ntest Loss: 0.1913 Acc: 0.9439\nEpoch 23/25\n----------\ntrain Loss: 0.3557 Acc: 0.8508\ntest Loss: 0.1828 Acc: 0.9346\nEpoch 24/25\n----------\ntrain Loss: 0.2677 Acc: 0.8952\ntest Loss: 0.2154 Acc: 0.8785\nEpoch 25/25\n----------\ntrain Loss: 0.2471 Acc: 0.8952\ntest Loss: 0.2100 Acc: 0.9159\nEpoch 1/25\n----------\ntrain Loss: 0.5286 Acc: 0.6976\ntest Loss: 0.3947 Acc: 0.7944\nEpoch 2/25\n----------\ntrain Loss: 0.4094 Acc: 0.8185\ntest Loss: 0.3841 Acc: 0.7757\nEpoch 3/25\n----------\ntrain Loss: 0.4386 Acc: 0.8185\ntest Loss: 0.4357 Acc: 0.8411\nEpoch 4/25\n----------\ntrain Loss: 0.3608 Acc: 0.8710\ntest Loss: 0.6367 Acc: 0.7664\nEpoch 5/25\n----------\ntrain Loss: 0.3672 Acc: 0.8347\ntest Loss: 0.3043 Acc: 0.8505\nEpoch 6/25\n----------\ntrain Loss: 0.3405 Acc: 0.8387\ntest Loss: 0.2565 Acc: 0.8598\nEpoch 7/25\n----------\ntrain Loss: 0.3047 Acc: 0.8589\ntest Loss: 0.2814 Acc: 0.8598\nEpoch 8/25\n----------\ntrain Loss: 0.2929 Acc: 0.8992\ntest Loss: 0.3152 Acc: 0.8505\nEpoch 9/25\n----------\ntrain Loss: 0.3210 Acc: 0.8468\ntest Loss: 0.2115 Acc: 0.8879\nEpoch 10/25\n----------\ntrain Loss: 0.2963 Acc: 0.8790\ntest Loss: 0.2113 Acc: 0.9065\nEpoch 11/25\n----------\ntrain Loss: 0.2537 Acc: 0.8911\ntest Loss: 0.1440 Acc: 0.8879\nEpoch 12/25\n----------\ntrain Loss: 0.2592 Acc: 0.8952\ntest Loss: 0.1365 Acc: 0.9439\nEpoch 13/25\n----------\ntrain Loss: 0.2044 Acc: 0.9194\ntest Loss: 0.2474 Acc: 0.8972\nEpoch 14/25\n----------\ntrain Loss: 0.2375 Acc: 0.8831\ntest Loss: 0.4458 Acc: 0.7944\nEpoch 15/25\n----------\ntrain Loss: 0.2389 Acc: 0.8992\ntest Loss: 0.4883 Acc: 0.7944\nEpoch 16/25\n----------\ntrain Loss: 0.2875 Acc: 0.8669\ntest Loss: 0.1994 Acc: 0.8879\nEpoch 17/25\n----------\ntrain Loss: 0.2278 Acc: 0.8992\ntest Loss: 0.2217 Acc: 0.9439\nEpoch 18/25\n----------\ntrain Loss: 0.2230 Acc: 0.9113\ntest Loss: 0.1830 Acc: 0.9252\nEpoch 19/25\n----------\ntrain Loss: 0.2112 Acc: 0.8871\ntest Loss: 0.1693 Acc: 0.9346\nEpoch 20/25\n----------\ntrain Loss: 0.2620 Acc: 0.9032\ntest Loss: 0.1859 Acc: 0.9439\nEpoch 21/25\n----------\ntrain Loss: 0.2414 Acc: 0.8790\ntest Loss: 0.1468 Acc: 0.9533\nEpoch 22/25\n----------\ntrain Loss: 0.2174 Acc: 0.9073\ntest Loss: 0.1025 Acc: 0.9626\nEpoch 23/25\n----------\ntrain Loss: 0.1911 Acc: 0.9194\ntest Loss: 0.1189 Acc: 0.9439\nEpoch 24/25\n----------\ntrain Loss: 0.1567 Acc: 0.9274\ntest Loss: 0.1245 Acc: 0.9346\nEpoch 25/25\n----------\ntrain Loss: 0.1477 Acc: 0.9476\ntest Loss: 0.1173 Acc: 0.9346\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Extract features using both models\u001b[39;00m\n\u001b[1;32m    127\u001b[0m mobilenet_features_train, train_labels \u001b[38;5;241m=\u001b[39m extract_features(mobilenet_model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 128\u001b[0m mobilenet_features_val, val_labels \u001b[38;5;241m=\u001b[39m extract_features(mobilenet_model, \u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    129\u001b[0m efficientnet_features_train, _ \u001b[38;5;241m=\u001b[39m extract_features(efficientnet_model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    130\u001b[0m efficientnet_features_val, _ \u001b[38;5;241m=\u001b[39m extract_features(efficientnet_model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","\u001b[0;31mKeyError\u001b[0m: 'val'"],"ename":"KeyError","evalue":"'val'","output_type":"error"}]},{"cell_type":"code","source":"# Extract features using both models\nmobilenet_features_train, train_labels = extract_features(mobilenet_model, dataloaders['train'])\nmobilenet_features_val, val_labels = extract_features(mobilenet_model, dataloaders['test'])\nefficientnet_features_train, _ = extract_features(efficientnet_model, dataloaders['train'])\nefficientnet_features_val, _ = extract_features(efficientnet_model, dataloaders['test'])\n\n# Stack the features\ntrain_features = np.hstack((mobilenet_features_train, efficientnet_features_train))\nval_features = np.hstack((mobilenet_features_val, efficientnet_features_val))\n\n# Define classifiers\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\nlr = LogisticRegression(max_iter=1000)\nsvc = SVC(probability=True)\n\n# Train the classifiers\nrf.fit(train_features, train_labels)\ngb.fit(train_features, train_labels)\nlr.fit(train_features, train_labels)\nsvc.fit(train_features, train_labels)\n\n# Validate the classifiers\nrf_preds = rf.predict(val_features)\ngb_preds = gb.predict(val_features)\nlr_preds = lr.predict(val_features)\nsvc_preds = svc.predict(val_features)\n\n# Print accuracy scores\nprint(\"Random Forest Accuracy:\", accuracy_score(val_labels, rf_preds))\nprint(\"Gradient Boosting Accuracy:\", accuracy_score(val_labels, gb_preds))\nprint(\"Logistic Regression Accuracy:\", accuracy_score(val_labels, lr_preds))\nprint(\"SVC Accuracy:\", accuracy_score(val_labels, svc_preds))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T07:08:49.125599Z","iopub.execute_input":"2024-09-11T07:08:49.125987Z","iopub.status.idle":"2024-09-11T07:09:01.650707Z","shell.execute_reply.started":"2024-09-11T07:08:49.125953Z","shell.execute_reply":"2024-09-11T07:09:01.649542Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Random Forest Accuracy: 0.9345794392523364\nGradient Boosting Accuracy: 0.9439252336448598\nLogistic Regression Accuracy: 0.9345794392523364\nSVC Accuracy: 0.9345794392523364\n","output_type":"stream"}]}]}